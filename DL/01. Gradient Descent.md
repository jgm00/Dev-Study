# Gradient Descent

## ëª©ì°¨
- [1. Gradient Descentë€?](#1-gradient-descentë€)
  - [ìˆ˜í•™ì  ê·¼ê±° (Taylor Approximation)](#ìˆ˜í•™ì -ê·¼ê±°-taylor-approximation)
- [2. Learning Rate (í•™ìŠµë¥ )](#2-learning-rate-í•™ìŠµë¥ )
  - [ì—­í• ](#ì—­í• )
  - [ë¬¸ì œì ](#ë¬¸ì œì ) 
  - [ê°œì„  ë°©ë²•](#ê°œì„ -ë°©ë²•)
- [3. Weight Initialization](#3-weight-initialization)
- [4. Variants of Gradient Descent](#4-variants-of-gradient-descent)
- [5. ë‹¨ì ](#5-ë‹¨ì )
  - [Local Minimum](#local-minimum)
  - [Saddle Point](#saddle-point) 
  - [Plateau / Flat Region](#plateau--flat-region)
- [6. í•´ê²° ì „ëµ](#6-í•´ê²°-ì „ëµ)


## 1. Gradient Descentë€?

![Gradient Descent](../assets/dl_img_1.png)

- ê³ ì°¨ì› ê³µê°„ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ ğ¿(ğœƒ)ë¥¼ ìµœì†Œí™”í•˜ëŠ” íŒŒë¼ë¯¸í„° ğœƒë¥¼ ì°¾ëŠ” ìµœì í™” ê¸°ë²•
- ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•´, ì†ì‹¤ì„ ê°€ì¥ ë¹ ë¥´ê²Œ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©í–¥ì„ ê³„ì‚°í•˜ê³ , ê·¸ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆì”© ì´ë™í•˜ë©° í•™ìŠµì„ ì§„í–‰
- GradientëŠ” ì†ì‹¤ í•¨ìˆ˜ê°€ ê°€ì¥ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ì•Œë ¤ì£¼ê¸° ë•Œë¬¸ì—, ê·¸ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ë©´ ì†ì‹¤ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒ


### ìˆ˜í•™ì  ê·¼ê±° (Taylor Approximation)

$L(\theta + \Delta\theta) \approx L(\theta) + \nabla_\theta L(\theta)^T \Delta\theta$

- ë§Œì•½ $\Delta\theta = -\eta \cdot \nabla_\theta L(\theta)$ ë¡œ ì„¤ì •í•˜ë©´, $L$ì´ ì¤„ì–´ë“¬

---

## 2. Learning Rate (í•™ìŠµë¥ )

### ì—­í• 
- Learning rate $\eta$ëŠ” í•œ ë²ˆ ì´ë™í•  ë•Œ ì–¼ë§Œí¼ ì›€ì§ì¼ì§€ë¥¼ ì¡°ì ˆí•¨

### ë¬¸ì œì 
- ë„ˆë¬´ í¬ë©´ : overshooting â†’ ë°œì‚°í•˜ê±°ë‚˜ global min ì§€ë‚˜ì¹¨
- ë„ˆë¬´ ì‘ìœ¼ë©´ : ëŠë¦° ìˆ˜ë ´, local minima/saddle pointì— ê°‡í˜

### ê°œì„  ë°©ë²•
- **Learning Rate Decay**: epoch ì§„í–‰ì— ë”°ë¼ ê°ì†Œ
- **Warmup + Decay**: ì´ˆë°˜ì—ëŠ” ì²œì²œíˆ ì¦ê°€ í›„ ê°ì†Œ
- **Adaptive Methods**:
  - **Adam**: í‰ê·  + ë¶„ì‚° ê³ ë ¤
  - **RMSProp**, **AdaGrad**: íŒŒë¼ë¯¸í„°ë³„ë¡œ ë‹¤ë¥¸ í•™ìŠµë¥ 

---

## 3. Weight Initialization

- ì´ˆê¸°ê°’ì— ë”°ë¼ gradientê°€ ì†Œì‹¤ë˜ê±°ë‚˜ í­ë°œí•  ìˆ˜ ìˆìŒ

### ì´ˆê¸°í™” ì „ëµ

- ëœë¤í•˜ê²Œ 0 ê·¼ì²˜ë¡œ ì¡ìŒ

| ì´ˆê¸°í™” ë°©ì‹ | ìˆ˜ì‹                                                                                                                  | ì£¼ë¡œ ì“°ëŠ” í™œì„±í™” í•¨ìˆ˜ |
|-------------|------------------------------------------------------------------------------------------------------------------------|------------------------|
| LeCun       | Normal: $\mathcal{N}(0, \frac{1}{n_{in}})$,  Uniform: $\mathcal{U}(-\sqrt{\frac{3}{n_{in}}}, \sqrt{\frac{3}{n_{in}}})$ | Sigmoid                |
| Xavier      | Normal: $\mathcal{N}(0, \frac{2}{n_{in} + n_{out}})$,  Uniform: $\mathcal{U}(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}})$ | Tanh                   |
| He          | Normal: $\mathcal{N}(0, \frac{2}{n_{in}})$,  Uniform: $\mathcal{U}(-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}})$ | ReLU ê³„ì—´              |


---

## 4. Variants of Gradient Descent

| ë°©ë²•                       | íŠ¹ì§•                          |
|----------------------------|-------------------------------|
| **Batch Gradient Descent** | ì „ì²´ ë°ì´í„° ê¸°ì¤€ â†’ ëŠë¦¼, ì•ˆì •ì  |
| **Stochastic Gradient Descent (SGD)** | ìƒ˜í”Œ 1ê°œ ê¸°ì¤€ â†’ ë¹ ë¦„, ë…¸ì´ì¦ˆ ìˆìŒ |
| **Mini-Batch Gradient Descent** | Nê°œì”© ë¬¶ìŒ â†’ ì†ë„-ì•ˆì •ì„± ê· í˜• |

---

## 5. ë‹¨ì 

### Local Minimum
- ì „ì²´ ìµœì†Ÿê°’ì´ ì•„ë‹Œ, ì§€ì—­ ìµœì†Ÿê°’ì— ê°‡íˆëŠ” í˜„ìƒ

### Saddle Point
- Gradientê°€ 0ì´ì§€ë§Œ ìµœì†Ÿê°’ì€ ì•„ë‹˜
- ê³ ì°¨ì› ê³µê°„ì—ì„œëŠ” ë” í”í•¨

### Plateau / Flat Region
- ê¸°ìš¸ê¸° ê±°ì˜ 0 â†’ **í•™ìŠµ ì •ì²´**

---

## 6. í•´ê²° ì „ëµ

| ë¬¸ì œ                 | í•´ê²° ê¸°ë²•                                 |
|----------------------|-------------------------------------------|
| Local Min / Saddle   | Momentum, Nesterov, Adam                  |
| Vanishing Gradient   | ReLU, BatchNorm, He Initialization       |
| Overshooting         | Learning Rate Decay, Gradient Clipping    |
| ëŠë¦° ìˆ˜ë ´            | Adaptive Optimizer, Learning Rate Warm-up |

---